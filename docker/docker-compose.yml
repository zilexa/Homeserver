## UPDATED: 03-6-2022, requires the NEW .env file!!
# changelog: https://github.com/zilexa/Homeserver/blob/master/docker/changelog.txt
##
## COMMON COMMANDS:
# Check for typos: docker-compose config 
# Run: docker-compose up -d 
# Stop a container: docker-compose stop containername
# Remove a stopped container: docker-compose rm containername 
# Stop all containers: docker kill $(docker ps -q)
# Remove stopped containers: docker container prune
# Stop & remove containers: docker rm $(docker ps -a -q) 
# Remove everything related to stopped containers: sudo docker system prune --all --volumes --force
#
## Docker-Compose 2.4 the final non-swarm version
#
version: "2.4"
services:
##_______SYSTEM_____
##____________________ Portainer [SYSTEM/Docker]
  portainer:
    container_name: portainer
    image: portainer/portainer-ce
    restart: always
    networks: 
      - web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - $DOCKERDIR/portainer/data:/data
    ports:
      - 9000:9000
    labels:
      caddy: http://docker.o
      caddy.reverse_proxy: "{{upstreams 9000}}"
      plugsy.name: Docker
      plugsy.link: http://docker.o/
      plugsy.category: System
##_____________________ Caddy [SYSTEM/web-proxy]
  caddy:
    container_name: web-proxy
    image: lucaslorentz/caddy-docker-proxy:ci-alpine
    restart: always
    networks: 
      - web-proxy
    environment:
      - CADDY_INGRESS_NETWORKS=web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - $DOCKERDIR/caddy/caddy_data:/data
      - $DOCKERDIR/caddy/config:/config
    extra_hosts:
      - host.docker.internal:host-gateway
    ports:
      - 443:443
      - 80:80
    labels:
      caddy.email: $EMAIL
      caddy_0: http://adguard.o
      caddy_0.reverse_proxy: host.docker.internal:3000
      caddy_1: http://vpn.o
      caddy_1.reverse_proxy: host.docker.internal:5000
      plugsy.name: Caddy web-proxy
      plugsy.category: System
##
##____________________ Plugsy [SYSTEM/Homepage]
  plugsy:
    container_name: plugsy
    image: plugsy/core
    restart: always
    networks: 
      - web-proxy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    #  - $DOCKERDIR/dashboard/config.json:/config.json
    ports:
      - 8000:3000
    labels:
      caddy: http://g.o
      caddy.reverse_proxy: "{{upstreams 3000}}"
##
##______PRIVACY_______
##______________________ AdGuard Home [PRIVACY/Blocker]
  adguard:
    container_name: adguard
    image: adguard/adguardhome
    restart: always
    network_mode: host
    volumes:
       - $DOCKERDIR/adguardhome/work:/opt/adguardhome/work
       - $DOCKERDIR/adguardhome//conf:/opt/adguardhome/conf
    labels:
      plugsy.name: AdGuard
      plugsy.link: http://adguard.o/
      plugsy.category: Privacy
##____________________ Unbound [PRIVACY/dns]
  unbound:
    container_name: unbound
    image: mvance/unbound:latest
    restart: always
    healthcheck:
      interval: 5m # to test the container, change to 10s. To prevent constant logfile activity, change to a few minutes
      timeout: 3s
      start_period: 5s
    networks: 
      - unbound
    volumes:
      - $DOCKERDIR/unbound/forward-records.conf:/opt/unbound/etc/unbound/forward-records.conf
    ports:
      - 5335:53/tcp
      - 5335:53/udp
    labels:
      plugsy.name: Unbound DNS
      plugsy.parents: AdGuard
##____________________ Castblock [PRIVACY/Chromecastblocker]
  castblock:
    container_name: castblock
    image: erdnaxeli/castblock:latest
    restart: always
    network_mode: host
    cap_add: 
      - NET_ADMIN
    environment:
      DEBUG: true
      OFFSET: 1
      CATEGORIES: sponsor,interaction
      MUTE_ADS: true
    labels:
      plugsy.name: CastBlock
      plugsy.parents: AdGuard
##
##_________ACCESS_______
##________________________ VPN-portal [ACCESS/vpn]
  VPN-portal:
    container_name: vpn-portal
    image: ngoduykhanh/wireguard-ui:latest
    restart: always
    cap_add:
      - NET_ADMIN
    network_mode: host
    environment:
      SESSION_SECRET: $WGPORTALSECRET
      WGUI_USERNAME: $USER
      WGUI_PASSWORD: $PW
      WGUI_CONFIG_FILE_PATH: /etc/wireguard/wg0.conf
      WGUI_ENDPOINT_ADDRESS: $DOMAIN
      WGUI_DNS: $WGIP
      WGUI_PERSISTENT_KEEPALIVE: 25
      WGUI_SERVER_INTERFACE_ADDRESSES: $WGIP/24
      WGUI_SERVER_LISTEN_PORT: $WGPORT
      WGUI_SERVER_POST_UP_SCRIPT: $WGPOSTUP
      WGUI_SERVER_POST_DOWN_SCRIPT: $WGPOSTDOWN
      WGUI_DEFAULT_CLIENT_ALLOWED_IPS: $WGIP/24
      SMTP_HOSTNAME: $SMTP
      SMTP_PORT: $SMTPPORT
      SMTP_USERNAME: $SMTPUSER
      SMTP_PASSWORD: $SMTPPASS
      SMTP_AUTH_TYPE: LOGIN
      EMAIL_FROM_ADDRESS: $RECIPIENT
      EMAIL_FROM_NAME: $SMTPUSER
    logging:
      driver: json-file
      options:
        max-size: 15m
    volumes:
      - $DOCKERDIR/vpn-portal/db:/app/db
      - /etc/wireguard:/etc/wireguard  
    labels:
      plugsy.name: VPN portal
      plugsy.link: http://vpn.o/
      plugsy.category: Access
##
##____________________ Guacamole [ACCESS/remote-desktop]
  guacamole:
    container_name: guacamole
    image: maxwaldorf/guacamole
    restart: always
    networks: 
      - web-proxy
    environment:
      EXTENSIONS: auth-quickconnect # add ,auth-totp if exposed to the internet, for 2FA
    volumes:
      - $DOCKERDIR/guacamole:/config
    ports:
     - 6000:8080
    labels:
      caddy: http://desktop.o
      caddy.reverse_proxy: "{{upstreams 8080}}"
      plugsy.name: Remote Desktop
      plugsy.link: http://desktop.o/
      plugsy.category: Access
##
##____________________Scrutiny [SYSTEM/Analytics]
  prometheus:
    image: prom/prometheus:v2.35.0
    container_name: mon-prometheus
    restart: always
    networks:
      - analytics
    user: root
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - $DOCKERDIR/analytics/prometheus:/etc/prometheus
      - $DOCKERDIR/analytics/prometheus_data:/prometheus
    ports:
      - 7000:9090 
  alertmanager:
    image: prom/alertmanager:v0.24.0
    container_name: mon-alertmanager
    networks:
      - analytics
    command:
      - '--config.file=/etc/alertmanager/config.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - $DOCKERDIR/analytics/alertmanager:/etc/alertmanager
    ports:
      - 9093:9093
  nodeexporter:
    image: prom/node-exporter:v1.3.1
    container_name: mon-nodeexporter
    restart: always
    network_mode: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - 9100:9100
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.44.0
    container_name: mon-cadvisor
    restart: always
    network_mode: host
    privileged: true
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /cgroup:/cgroup:ro #doesn't work on MacOS only for Linux
    ports:
      - 8080:8080
  grafana:
    image: grafana/grafana:8.4.5
    container_name: mon-grafana
    networks:
      - analytics
    user: root
    volumes:
      - $DOCKERDIR/analytics/grafana/data:/var/lib/grafana
      - $DOCKERDIR/analytics/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - $DOCKERDIR/analytics/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: unless-stopped
    ports:
      - 4000:3000
  pushgateway:
    image: prom/pushgateway:v1.4.2
    container_name: mon-pushgateway
    restart: unless-stopped
    networks:
      - analytics
    ports:
      - 9091:9091
##
##________CLOUD______
##_____________________ Firefox Sync [CLOUD/Browser] 
# generate secret.txt first see docker-config.sh
  firefox-sync:
    container_name: firefox-sync
    image: crazymax/firefox-syncserver:latest
    restart: always
    networks: 
      - web-proxy
    environment: 
      FF_SYNCSERVER_PUBLIC_URL: https://firefox.$DOMAIN
      FF_SYNCSERVER_SECRET: $FFSYNCSECRET
      FF_SYNCSERVER_FORWARDED_ALLOW_IPS: '*'
      FF_SYNCSERVER_FORCE_WSGI_ENVIRON: true
      FF_SYNCSERVER_ALLOW_NEW_USERS: false
      FF_SYNCSERVER_LOGLEVEL: debug
      FF_SYNCSERVER_ACCESSLOG: true
    volumes:
      - $DOCKERDIR/firefox-sync:/data
    labels:
      caddy: firefox.$DOMAIN
      caddy.reverse_proxy: "{{upstreams 5000}}"
      plugsy.name: Firefox Sync
      plugsy.category: Cloud
##
##_____________________ Bitwarden [CLOUD/Password-manager] 
  vaultwarden:
    container_name: vaultwarden
    image: vaultwarden/server
    restart: always
    healthcheck:
      interval: 5m # to test the container, change to 10s. To prevent constant logfile activity, change to a few minutes
    networks: 
      - web-proxy
    volumes:
      - $DOCKERDIR/vaultwarden:/data
    environment:
      WEBSOCKET_ENABLED: true
      DOMAIN: vault.$DOMAIN
      SIGNUPS_ALLOWED: false
      ADMIN_TOKEN: $VAULTWARDENTOKEN
    labels:
      caddy: vault.$DOMAIN
      caddy.reverse_proxy_0: "{{upstreams 80}}"
      # Required extra headers
      caddy.encode: gzip
      caddy.header.X-XSS-Protection: '"1; mode=block;"'
      caddy.header.X-Frame-Options: "DENY"
      caddy.header.X-Content-Type-Options: "none"
      caddy.reverse_proxy_1: "/notifications/hub/negotiate {{upstreams 80}}"
      caddy.reverse_proxy_2: "/notifications/hub {{upstreams 3012}}"
      plugsy.name: Vaultwarden
      plugsy.link: https://vault.$DOMAIN
      plugsy.category: Cloud
##
##____________________ FileRun [CLOUD/FileRun]
  filerun:
    container_name: filerun
    image: filerun/filerun
    restart: always
    networks: 
      - web-proxy
      - filerun
    environment:
      FR_DB_HOST: filerun-db
      FR_DB_PORT: 3306
      FR_DB_NAME: filerundb
      FR_DB_USER: $USER
      FR_DB_PASS: $PW_DB
      APACHE_RUN_USER: $USER
      APACHE_RUN_USER_ID: $PUID
      APACHE_RUN_GROUP: $USER
      APACHE_RUN_GROUP_ID: $PGID
    depends_on:
      - filerun-db
    volumes:
      - $DOCKERDIR/filerun/html:/var/www/html
      - $DATAPOOL/Users:/user-files
    labels:
      caddy: files.$DOMAIN
      caddy.reverse_proxy: "{{upstreams 80}}"
      caddy.reverse_proxy.header_up: "Host files.$DOMAIN"
      # Required extra headers
      caddy.file_server: ""                                         # required for fileservers
      caddy.encode: gzip                                            # required for fileservers
      caddy.header.Strict-Transport-Security: '"max-age=15768000;"' # Recommended security hardening for fileservers
      caddy.header.X-XSS-Protection: '"1; mode=block;"'             # Recommended security hardening for fileservers
      caddy.header.X-Content-Type-Options: "nosniff"                # Seems required to open files in OnlyOffice
      caddy.header.X-Frame-Options: "SAMEORIGIN"                    # Seems required to open files in OnlyOffice
      plugsy.name: FileRun
      plugsy.link: https://files.$DOMAIN
      plugsy.category: Cloud
##____________________ Filerun database [CLOUD/FileRun/db]
  filerun-db:
    container_name: filerun-db
    image: mariadb:10.1
    restart: always
    networks:
      - filerun
    environment:
      MYSQL_ROOT_PASSWORD: $PW_DB
      MYSQL_USER: $USER
      MYSQL_PASSWORD: $PW_DB
      MYSQL_DATABASE: filerundb
    volumes:
      - $DOCKERDIR/filerun/db:/var/lib/mysql
    labels:
      plugsy.name: FileRun Database
      plugsy.parents: FileRun
##
##
##________MEDIA________
##_____________________ Jellyfin [MEDIA/Library] 
  jellyfin:
    container_name: jellyfin
    image: cr.hotio.dev/hotio/jellyfin
    restart: always
    networks: 
      - web-proxy
    # Required for Intel QuickSync/VAAPI hardware accelerated video encoding/transcoding
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    environment:
      PUID: $PUID
      PGID: $PGID
      TZ: $TZ
      UMASK_SET: 002 #optional
    volumes:
      - $DOCKERDIR/jellyfin/config:/config
      - $DATAPOOL/Media:/data
    ports:
      - 8096:8096
    labels:
      caddy: http://jellyfin.o
      caddy.reverse_proxy: "{{upstreams 8096}}"
      plugsy.name: Jellyfin
      plugsy.link: http://jellyfin.o/
      plugsy.category: Media 
      org.hotio.pullio.update: true

##____________________ VPN-proxy [MEDIA/vpn-client-for-media]
  VPN-proxy:
    container_name: VPN-proxy
    image: thrnz/docker-wireguard-pia
    restart: always
    networks: 
      - web-proxy
    cap_add:
      - NET_ADMIN
      #- SYS_MODULE might not be needed with a 5.6+ kernel?
      #- SYS_MODULE
      # Mounting the tun device may be necessary for userspace implementations
      #devices:
      #- /dev/net/tun:/dev/net/tun
    privileged: true
    sysctls:
      # wg-quick fails to set this without --privileged, so set it here instead if needed
      - net.ipv4.conf.all.src_valid_mark=1
      # May as well disable ipv6. Should be blocked anyway.
      - net.ipv6.conf.default.disable_ipv6=1
      - net.ipv6.conf.all.disable_ipv6=1
      - net.ipv6.conf.lo.disable_ipv6=1
    healthcheck:
      test: ping -c 1 www.google.com || exit 1
      interval: 5m # While testing container, change to 10s. To prevent constant logfile activity, change to a few minutes
      timeout: 10s
      start_period: 10s
      retries: 3
    environment:
      LOCAL_NETWORK: 192.168.88.0/24,10.6.0.1/24
      LOC: de-frankfurt
      USER: $VPN_USER_PIA
      PASS: $VPN_PW_PIA
      #KEEPALIVE: 25
      #VPNDNS: 8.8.8.8,8.8.4.4
      PORT_FORWARDING: 1
      PORT_PERSIST: 0
      PORT_SCRIPT: /pia-shared/updateport-qbittorrent.sh
      #WG_USERSPACE: 1
    volumes:
      # Auth token is stored here
      - $DOCKERDIR/vpn-proxy/pia:/pia
      # If enabled, the forwarded port is dumped to /pia-shared/port.dat for potential use in other containers
      - $DOCKERDIR/vpn-proxy/pia-shared:/pia-shared
    # The container has no recovery logic. Use a healthcheck to catch disconnects.
    ports:
      - 9090:8080 #Qbittorrent webUI
    labels:
      org.hotio.pullio.update: true
      caddy: http://downloads.o
      caddy.reverse_proxy: "{{upstreams 8080}}"
      plugsy.name: VPN-Proxy
      plugsy.parents: Downloads
##
##____________________ Transmission [MEDIA/download-client]
  qbittorrent:
    container_name: qbittorrent
    image: cr.hotio.dev/hotio/qbittorrent
    depends_on:
      - VPN-proxy
    network_mode: service:VPN-proxy
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      TZ: $TZ
    volumes:
      - $DOCKERDIR/qbittorrent:/config
      - $DATAPOOL/Media/incoming:/Media/incoming
    labels: 
      org.hotio.pullio.update: true
      plugsy.name: Downloads
      plugsy.link: http://downloads.o/
      plugsy.category: Media

##
##____________________ Prowlarr [MEDIA/torrent-proxy for Sonarr&Radarr]
  prowlarr:
    container_name: prowlarr
    image: cr.hotio.dev/hotio/prowlarr:testing
    networks: 
      - web-proxy
    depends_on:
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/prowlarr/config:/config
      - $DATAPOOL/Media/incoming:/Media/incoming
    ports:
      - 9696:9696
    labels:
      caddy: http://torrents.o
      caddy.reverse_proxy: "{{upstreams 9696}}"
      plugsy.name: Search Torrents
      plugsy.link: http://torrents.o/
      plugsy.category: Media
      org.hotio.pullio.update: true
##
##____________________ Sonarr [MEDIA/PVR-TVshows]
  sonarr:
    container_name: sonarr
    image: cr.hotio.dev/hotio/sonarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/sonarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 8989:8989
    labels:
      caddy: http://sonarr.o
      caddy.reverse_proxy: "{{upstreams 8989}}"
      plugsy.name: Sonarr (series)
      plugsy.link: http://sonarr.o/
      plugsy.category: Media
      org.hotio.pullio.update: true
##
##____________________ Radarr [MEDIA/PVR-Movies]
  radarr:
    container_name: radarr
    image: cr.hotio.dev/hotio/radarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/radarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 7878:7878
    labels:
      caddy: http://radarr.o
      caddy.reverse_proxy: "{{upstreams 7878}}"
      plugsy.name: Radarr (movies)
      plugsy.link: http://radarr.o/
      plugsy.category: Media 
      org.hotio.pullio.update: true
##
##____________________ Bazarr [MEDIA/subtitles]
  bazarr:
    container_name: bazarr
    image: cr.hotio.dev/hotio/bazarr
    networks: 
      - web-proxy
    depends_on:
       - sonarr
       - radarr
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/bazarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 6767:6767
    labels:
      caddy: http://bazarr.o
      caddy.reverse_proxy: "{{upstreams 6767}}"
      plugsy.name: Bazarr (subtitles)
      plugsy.link: http://bazarr.o/
      plugsy.category: Media
      org.hotio.pullio.update: true
##
##____________________ Lidarr [MEDIA/PVR-Music]
  lidarr:
    container_name: lidarr
    image: cr.hotio.dev/hotio/lidarr
    networks: 
      - web-proxy
    depends_on:
      - prowlarr
      - qbittorrent
    restart: always
    environment:
      PUID: $PUID
      PGID: $PGID
      UMASK: 002
      TZ: $TZ
    volumes:
      - $DOCKERDIR/lidarr/config:/config
      - $DATAPOOL/Media:/Media
    ports:
      - 8686:8686
    labels:
      caddy: http://lidarr.o
      caddy.reverse_proxy: "{{upstreams 8686}}"
      plugsy.name: Lidarr (music)
      plugsy.link: http://lidarr.o/
      plugsy.category: Media
      org.hotio.pullio.update: true
#
#
networks:
  web-proxy:
    external: true
  analytics:
    driver: bridge
  filerun:
    driver: bridge
  unbound:
    driver: bridge
